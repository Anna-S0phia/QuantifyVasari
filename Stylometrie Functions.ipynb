{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Stylometrie Function version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the Vasari files\n",
    "datapath = ('./data/results/Vasari_letters_vite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# several functions to compiles all of the text files  \n",
    "# associated with a single author or unkown into a single string\n",
    "# \n",
    "import os\n",
    "import nltk\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def read_files_into_string(datapath, filenames):\n",
    "    strings = []\n",
    "    for file in filenames:\n",
    "        with open(datapath + '/' + file) as f:\n",
    "            strings.append(f.read())\n",
    "    return '\\n'.join(strings)\n",
    "\n",
    "\n",
    "# function to build list of filename per author\n",
    "def get_filenames_per_author_into_dict(datapath):\n",
    "    list_of_files = os.listdir(datapath)\n",
    "    store = list()\n",
    "    for i in list_of_files:\n",
    "        store.append(i.split('_')[0])\n",
    "    authors = list(set(store))\n",
    "    storage = {}\n",
    "    count = 0\n",
    "    for a in authors:\n",
    "        store2 = list()\n",
    "        for f in list_of_files:\n",
    "            if f.startswith(a):\n",
    "                store2.append(f)\n",
    "            else:\n",
    "                continue\n",
    "        storage[authors[count]] = store2\n",
    "        count += 1\n",
    "        \n",
    "    return storage\n",
    "\n",
    "# put every author into a single string in a dict\n",
    "def get_single_string_per_author_into_dict(datapath):\n",
    "    x = get_filenames_per_author_into_dict(datapath)\n",
    "    authors = list(x.keys())\n",
    "    strings_by_author = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        strings_by_author[author] = read_files_into_string(datapath, x[author])\n",
    "\n",
    "    return strings_by_author\n",
    "\n",
    "\n",
    "# transform authors' corpora into lists of word tokens\n",
    "def get_tokens_by_author(datapath):\n",
    "    # call function to process data first\n",
    "    strings = get_single_string_per_author_into_dict(datapath)\n",
    "    authors = strings.keys()\n",
    "    \n",
    "    # storage variables\n",
    "    length_distribution = []\n",
    "    sorted_length = []\n",
    "    author_tokens = {}\n",
    "    author_length_distributions = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        tokens = nltk.word_tokenize(strings[author], language = 'italian')\n",
    "    \n",
    "    # Filter out punctuation\n",
    "        author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "    # Get a distribution of token lengths\n",
    "        token_lengths = [len(token) for token in author_tokens[author]]\n",
    "        author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "    #author_length_distributions[author].plot(15,title=author)\n",
    "        sorted_length.append((dict(sorted((dict(author_length_distributions[author])).items()))))\n",
    "        length_distribution.append(author_length_distributions[author])\n",
    "    \n",
    "    return author_tokens\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_tokens_by_author_extended(datapath):\n",
    "    # call function to process data first\n",
    "    strings = get_single_string_per_author_into_dict(datapath)\n",
    "    authors = strings.keys()\n",
    "    \n",
    "    # storage variables\n",
    "    length_distribution = []\n",
    "    sorted_length = []\n",
    "    author_tokens = {}\n",
    "    author_length_distributions = {}\n",
    "    \n",
    "    for author in authors:\n",
    "        tokens = nltk.word_tokenize(strings[author], language = 'italian')\n",
    "    \n",
    "    # Filter out punctuation\n",
    "        author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "    # Get a distribution of token lengths\n",
    "        token_lengths = [len(token) for token in author_tokens[author]]\n",
    "        author_length_distributions[author] = nltk.FreqDist(token_lengths)\n",
    "    #author_length_distributions[author].plot(15,title=author)\n",
    "        sorted_length.append((dict(sorted((dict(author_length_distributions[author])).items()))))\n",
    "        length_distribution.append(author_length_distributions[author])\n",
    "    \n",
    "    return {\"length_distribution\": length_distribution, \"sorted_length\": sorted_length, \"author_tokens\": author_tokens, \"author_length_distributions\": author_length_distributions}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Deltascore function\n",
    "\n",
    "def get_delta_score(datapath,\n",
    "                    author_candidates = ('Vasari', 'Borghini'),\n",
    "                   disputed = 'unkown',\n",
    "                   X = 500, \n",
    "                   language = 'italian'):\n",
    "    \"\"\"input variables: \n",
    "    datapath, \n",
    "    author_candidates(default = ('Vasari', 'Borghini')), \n",
    "    disputed_text(default = 'unkown') named'unkown'\n",
    "    X(default = 500) = number of the most common words to use as features\n",
    "    language(default = 'italian')\"\"\"\n",
    " \n",
    "    # set all tokens lower case\n",
    "    tokens_by_author = get_tokens_by_author(datapath)\n",
    "    for author in author_candidates:\n",
    "        tokens_by_author[author] = ([tok.lower() for tok in tokens_by_author[author]])\n",
    "        \n",
    "    # Combine every paper except our test case into a single corpus\n",
    "    whole_corpus = []\n",
    "    for author in author_candidates:\n",
    "        whole_corpus += tokens_by_author[author]\n",
    "        \n",
    "    # Get a frequency distribution\n",
    "    whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(X)) # ! # X VARIABLE here! <--------- !\n",
    "\n",
    "    # Calculating features for each subcorpus\n",
    "    # The main data structure\n",
    "    features = [word for word,freq in whole_corpus_freq_dist]\n",
    "    feature_freqs = {}\n",
    "\n",
    "    for author in author_candidates:\n",
    "    # A dictionary for each candidate's features\n",
    "        feature_freqs[author] = {}\n",
    "\n",
    "    # A helper value containing the number of tokens in the author's subcorpus\n",
    "        overall = len(tokens_by_author[author])\n",
    "\n",
    "    # Calculate each feature's presence in the subcorpus\n",
    "        for feature in features:\n",
    "            presence = tokens_by_author[author].count(feature)\n",
    "            feature_freqs[author][feature] = presence / overall\n",
    "            \n",
    "    # Calculating feature averages and standard deviations\n",
    "\n",
    "    # The data structure into which we will be storing the \"corpus standard\" statistics\n",
    "    corpus_features = {}\n",
    "\n",
    "    # For each feature...\n",
    "    for feature in features:\n",
    "    # Create a sub-dictionary that will contain the feature's mean\n",
    "    # and standard deviation\n",
    "        corpus_features[feature] = {}\n",
    "    \n",
    "    # Calculate the mean of the frequencies expressed in the subcorpora\n",
    "        feature_average = 0\n",
    "        for author in author_candidates:\n",
    "            feature_average += feature_freqs[author][feature]\n",
    "        feature_average /= len(author_candidates)\n",
    "        corpus_features[feature][\"Mean\"] = feature_average\n",
    "\n",
    "    # Calculate the standard deviation using the basic formula for a sample\n",
    "        feature_stdev = 0\n",
    "        for author in author_candidates:\n",
    "            diff = feature_freqs[author][feature] - corpus_features[feature][\"Mean\"]\n",
    "            feature_stdev += diff*diff\n",
    "        feature_stdev /= (len(author_candidates) - 1)\n",
    "        feature_stdev = math.sqrt(feature_stdev)\n",
    "        corpus_features[feature][\"StdDev\"] = feature_stdev\n",
    "    \n",
    "    # Calculating z-scores\n",
    "\n",
    "    feature_zscores = {}\n",
    "    for author in author_candidates:\n",
    "        feature_zscores[author] = {}\n",
    "        for feature in features:\n",
    "\n",
    "        # Z-score definition = (value - mean) / stddev\n",
    "        # We use intermediate variables to make the code easier to read\n",
    "            feature_val = feature_freqs[author][feature]\n",
    "            feature_mean = corpus_features[feature][\"Mean\"]\n",
    "            feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "            feature_zscores[author][feature] = ((feature_val-feature_mean) /\n",
    "                                            feature_stdev)\n",
    "    \n",
    "    \n",
    "    # Calculating features and z-scores for the \"unkown\" = disputed biographies\n",
    "\n",
    "    # Tokenize the test case\n",
    "    testcase_tokens = tokens_by_author[disputed]\n",
    "\n",
    "    # Calculate the test case's features\n",
    "    overall = len(testcase_tokens)\n",
    "    testcase_freqs = {}\n",
    "    for feature in features:\n",
    "        presence = testcase_tokens.count(feature)\n",
    "        testcase_freqs[feature] = presence / overall\n",
    "\n",
    "    # Calculate the test case's feature z-scores\n",
    "    testcase_zscores = {}\n",
    "    for feature in features:\n",
    "        feature_val = testcase_freqs[feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev\n",
    "        #print(\"Test case z-score for feature\", feature, \"is\", testcase_zscores[feature])\n",
    "    \n",
    "    print('the number of features is: ', len(features))  \n",
    "    \n",
    "    # Calculate Delta\n",
    "    storage = { \"X\": X}\n",
    "    for author in author_candidates:\n",
    "        delta = 0\n",
    "        for feature in features:\n",
    "            delta += math.fabs((testcase_zscores[feature] -\n",
    "                                feature_zscores[author][feature]))\n",
    "        delta /= len(features)\n",
    "        storage[author] = delta\n",
    "        print( \"- for \", disputed, \" the Delta score for candidate\", author, \"is\", round(delta,4) )\n",
    "    \n",
    "    print('\\nthe nltk.FreqDist(whole_corpus).most_common(X) value was: ', X, '; tokenizer: language =', language) \n",
    "    return storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of features is:  500\n",
      "- for  Vasari  the Delta score for candidate Bartoli is 0.9357\n",
      "- for  Vasari  the Delta score for candidate Borghini is 0.914\n",
      "- for  Vasari  the Delta score for candidate Giambullari is 1.2889\n",
      "- for  Vasari  the Delta score for candidate Vcopy is 0.6136\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  500 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 500,\n",
       " 'Bartoli': 0.9357027495520863,\n",
       " 'Borghini': 0.9139842885525475,\n",
       " 'Giambullari': 1.2888761965891056,\n",
       " 'Vcopy': 0.6135980418770737}"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath,('Bartoli', 'Borghini', 'Giambullari', 'Vcopy'), disputed = 'Vasari', X = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of features is:  50\n",
      "- for  Vasari  the Delta score for candidate Bartoli is 1.1419\n",
      "- for  Vasari  the Delta score for candidate Borghini is 1.1717\n",
      "- for  Vasari  the Delta score for candidate Giambullari is 1.4253\n",
      "- for  Vasari  the Delta score for candidate Vcopy is 0.575\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  50 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 50,\n",
       " 'Bartoli': 1.1418627181352425,\n",
       " 'Borghini': 1.1717097713405429,\n",
       " 'Giambullari': 1.4252578269243714,\n",
       " 'Vcopy': 0.5749756716639481}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath,('Bartoli', 'Borghini', 'Giambullari', 'Vcopy'), disputed = 'Vasari', X = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of features is:  50\n",
      "- for  unkown  the Delta score for candidate Bartoli is 1.4788\n",
      "- for  unkown  the Delta score for candidate Borghini is 1.845\n",
      "- for  unkown  the Delta score for candidate Giambullari is 2.125\n",
      "- for  unkown  the Delta score for candidate Vasari is 1.6158\n",
      "- for  unkown  the Delta score for candidate Vcopy is 1.7602\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  50 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 50,\n",
       " 'Bartoli': 1.4788289405455124,\n",
       " 'Borghini': 1.845040173049742,\n",
       " 'Giambullari': 2.1250284323044517,\n",
       " 'Vasari': 1.6158125561561507,\n",
       " 'Vcopy': 1.7601945555582637}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath,('Bartoli', 'Borghini', 'Giambullari', 'Vasari', 'Vcopy'), X = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the number of features is:  50\n",
      "- for  unkown  the Delta score for candidate Vasari is 4.0188\n",
      "- for  unkown  the Delta score for candidate Borghini is 4.1429\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  50 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 50, 'Vasari': 4.0187661559418855, 'Borghini': 4.142939558936843}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath, X = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- for  unkown  the Delta score for candidate Vasari is 6.39789960050083\n",
      "- for  unkown  the Delta score for candidate Borghini is 6.427139171358765\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  500 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 500, 'Vasari': 6.39789960050083, 'Borghini': 6.427139171358765}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#?????test = get_tokens_p_author_into_dict(datapath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- for  unkown  the Delta score for candidate Bartoli is 1.2377323231396937\n",
      "- for  unkown  the Delta score for candidate Borghini is 1.2635584021906843\n",
      "- for  unkown  the Delta score for candidate Giambullari is 1.0613820175750421\n",
      "- for  unkown  the Delta score for candidate Vasari is 1.1595709261708145\n",
      "- for  unkown  the Delta score for candidate Vcopy is 1.2607271753521416\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  7000 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 7000,\n",
       " 'Bartoli': 1.2377323231396937,\n",
       " 'Borghini': 1.2635584021906843,\n",
       " 'Giambullari': 1.0613820175750421,\n",
       " 'Vasari': 1.1595709261708145,\n",
       " 'Vcopy': 1.2607271753521416}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath, author_candidates = ('Bartoli', 'Borghini', 'Giambullari', 'Vasari', 'Vcopy'), X = 7000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- for  unkown  the Delta score for candidate Bartoli is 1.4220630660309481\n",
      "- for  unkown  the Delta score for candidate Borghini is 1.4885549299182328\n",
      "- for  unkown  the Delta score for candidate Giambullari is 1.8512282543585277\n",
      "- for  unkown  the Delta score for candidate Vasari is 1.3195617334711442\n",
      "- for  unkown  the Delta score for candidate Vcopy is 1.4504749606798273\n",
      "- for  unkown  the Delta score for candidate unkown is 0.0\n",
      "\n",
      "the nltk.FreqDist(whole_corpus).most_common(X) value was:  100 ; tokenizer: language = italian\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'X': 100,\n",
       " 'Bartoli': 1.4220630660309481,\n",
       " 'Borghini': 1.4885549299182328,\n",
       " 'Giambullari': 1.8512282543585277,\n",
       " 'Vasari': 1.3195617334711442,\n",
       " 'Vcopy': 1.4504749606798273,\n",
       " 'unkown': 0.0}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_delta_score(datapath, author_candidates = ('Bartoli', 'Borghini', 'Giambullari', 'Vasari', 'Vcopy', 'unkown'), X = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function get_delta_score in module __main__:\n",
      "\n",
      "get_delta_score(datapath, author_candidates=('Vasari', 'Borghini'), disputed='unkown', X=500, language='italian')\n",
      "    input variables: \n",
      "    datapath, \n",
      "    author_candidates(default = ('Vasari', 'Borghini')), \n",
      "    disputed_text(default = 'unkown') named'unkown'\n",
      "    X(default = 500) = number of the most common words to use as features\n",
      "    language(default = 'italian')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(get_delta_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## count 'disegn' and 'rinasci'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VASARI mentioned 'disegn'235 times.\n",
      " Relative to textlength:0.0016193271867807775\n",
      "Vasari mentioned 'rinasc'1 times.\n",
      "['rinascere']\n",
      "BORGHINI mentioned 'disegn'138 times.\n",
      " Relative to textlength:0.0016928153481924903\n",
      "Borghini mentioned 'rinasc'1 times.\n",
      "['rinascente']\n",
      "BARTOLI mentioned 'disegn'16 times.\n",
      " Relative to textlength:0.0007379733407130668\n",
      "Bartoli mentioned 'rinasc'0 times.\n",
      "[]\n",
      "UNKOWN mentioned 'disegn'1831 times.\n",
      " Relative to textlength:0.0025917260102564974\n",
      "unkown mentioned 'rinasc'6 times.\n",
      "['rinascimento', 'rinasceva', 'rinascere', 'rinascita', 'rinascita', 'rinascita']\n",
      "GIAMBULLARI mentioned 'disegn'5 times.\n",
      " Relative to textlength:0.002508780732563974\n",
      "Giambullari mentioned 'rinasc'0 times.\n",
      "[]\n",
      "VCOPY mentioned 'disegn'23 times.\n",
      " Relative to textlength:0.0017074981440237565\n",
      "Vcopy mentioned 'rinasc'0 times.\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "storage = {}\n",
    "for i in x['author_tokens']:\n",
    "    disegno = []\n",
    "    xdisegn = []\n",
    "    rinascita = []\n",
    "    for t in x['author_tokens'][i]:\n",
    "        if t.startswith('disegn'):\n",
    "            disegno.append(t)\n",
    "        elif 'disegn' in t:\n",
    "            xdisegn.append(t)\n",
    "        elif t.startswith('rinasc'):\n",
    "            rinascita.append(t)\n",
    "        else:\n",
    "            continue\n",
    "    storage[i] = {'disegno': disegno, 'rinascita': rinascita }\n",
    "    print(str(i).upper() + ' mentioned \\'disegn\\'' + str(len(disegno)) + ' times.\\n Relative to textlength:' + str((len(disegno)/len(x['author_tokens'][i]))))\n",
    "    print(str(i) + ' mentioned \\'rinasc\\'' + str(len(rinascita)) + ' times.\\n'  + str(rinascita))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vasari 15 \n",
      "\n",
      "Borghini 62 \n",
      "\n",
      "Bartoli 3 \n",
      "\n",
      "unkown 948 \n",
      "\n",
      "Giambullari 3 \n",
      "\n",
      "Vcopy 1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "storage = {}\n",
    "for i in x['author_tokens']:\n",
    "    disegno = []\n",
    "    for t in x['author_tokens'][i]:\n",
    "        if t.startswith('disegno'):\n",
    "            disegno.append(t)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    storage[i] = disegno    \n",
    "    print(i, len(disegno),  '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
